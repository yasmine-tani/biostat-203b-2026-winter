---
title: "biostat212hw3"
format: html
editor: visual
author: Yasmine Tani; UID 406579988
---

# Biostatistics 212A - Homework 3

## 1-5.4.2 ISL Exercise

We will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of n observations.

**(a) What is the probability that the first bootstrap observation is not the jth observation from the original sample? Justify your answer.**

The probability of selecting j is $1/n$ ; therefore $P(notJ) = 1-(1/n)$ . Each obs. has equal chance of being selected, so we are looking at the complement.

------------------------------------------------------------------------

**(b) What is the probability that the second bootstrap observation is not the jth observation from the original sample?**

Bootstrap sampling is independent with replacement. So, the second draw has the same probability of $P(notJ) = 1-(1/n)$.

------------------------------------------------------------------------

**(c) Argue that the probability that the jth observation is not in the bootstrap sample is** $(1-(1/n))^n$ **.**

To create a bootstrap sample, we perform n independent draws. for the *jth observation* to be completely mission from the bootstrap sample, it must not be picked in the 1st, 2nd, or.... nth draw. So we multiple $P(notJ) = 1-(1/n)$ n times which is the same as $(1-(1/n))^n$.

------------------------------------------------------------------------

**(d) When n= 5, what is the probability that the jth observation is in the bootstrap sample?**

Ok, so let us first find the probability that it is **not in the sample**.

$(1-(1/n))^n$ with $n=5$ gets us $(4/5)^5 = 0.32768$. Therefore, the complement of $1-0.32768=0.67232$ gives us the probability that the jth observation **is** in the bootstrap sample when n=5.

------------------------------------------------------------------------

**(e) When n= 100, what is the probability that the jth observation is in the bootstrap sample?**

Same logic.
$(1-(1/n))^n$ with $n=100$ gets us $(99/100)^100 = 0.366$. Therefore, the complement of $1-0.366=0.634$ gives us the probability that the jth observation **is** in the bootstrap sample when n=100.

------------------------------------------------------------------------

**(f) When n= 10,000, what is the probability that the jth observation is in the bootstrap sample?**

$(1-(1/n))^n$ with $n=10,000$ gets us $(1-(1/10000))^10000 = 0.3679$. Therefore, the complement of $1-0.3679=0.632$ gives us the probability that the jth observation **is** in the bootstrap sample when n=10,000.

------------------------------------------------------------------------

**(g) Create a plot that displays, for each integer value of n from 1 to 100,000, the probability that the jth observation is in the bootstrap sample. Comment on what you observe.**

```{r}
#  sequence of n values
n <- 1:100000

#  probabilities
p <- 1 - (1 - 1/n)^n

# plotttttt
plot(n, p,
     type = "l",
     xlab = "n",
     ylab = "Probability j is in bootstrap sample",
     main = "Bootstrap Inclusion Probability")

# theoretical limit line
abline(h = 0.632, lty = 2)
```

The probability that the jth observation appears in the bootstrap sample increases quickly as n increases and approaches approximately 0.632. The curve stabilizes near this value for large n, confirming the theoretical limit $1-e^{-1} ≈ 0.632$ .

------------------------------------------------------------------------

**(h) We will now investigate numerically the probability that a bootstrap sample of size n= 100 contains the jth observation. Here j = 4. We repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample.**

```{r}
store <- rep(NA, 10000)

for(i in 1:10000){
  store[i] <- sum(sample(1:100, rep=TRUE) == 4) > 0
}

mean(store)
```

The value obtained is very close to 0.63. In part f, we ran the simulation 10,000 times and due to the law of large numbers , the experimental mean should be extremely close to the theoretical value and this was the case. and confirms our derivation.

------------------------------------------------------------------------

## 2-5.4.9 ISL Exercise

**(a) Based on this data set, provide an estimate for the population mean of medv. Call this estimate ˆ µ.**

```{r}
library(ISLR2)
data(Boston)
mu_hat <- mean(Boston$medv)
mu_hat
```

The estimate for the population mean of medv is the sample mean:

$\hat{\mu}≈22.53$

------------------------------------------------------------------------

**(b)** Provide an estimate of the standard error of ˆ µ. Interpret this result.

*Hint: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations.*

```{r}
se_hat <- sd(Boston$medv) / sqrt(nrow(Boston))
se_hat
```

The estimated standard error is
$SE(\hat{\mu}) \approx 0.41$.

------------------------------------------------------------------------

**(c) Now estimate the standard error of ˆ µusing the bootstrap. How does this compare to your answer from (b)?**

```{r}
library(boot)
boot_mean <- function(data, index){
  mean(data[index])
}
boot_results <- boot(Boston$medv, boot_mean, R=1000)
boot_results
boot_results$t0
sd(boot_results$t)
```

The bootstrap estimate of the standard error is approximately 0.41, which is very close to the estimate obtained in part b!

------------------------------------------------------------------------

**(d) Based on your bootstrap estimate from (c), provide a 95 % confidence interval for the mean of medv. Compare it to the results obtained using t.test(Boston\$medv).**

*Hint: You can approximate a 95 % confidence interval using the formula* $[\hat{\mu} -2SE(\hat{\mu}),\hat{\mu}+2SE(\hat{\mu})]$ *.*

```{r}
mu_hat - 2*sd(boot_results$t)
mu_hat + 2*sd(boot_results$t)
```

```{r}
t.test(Boston$medv)
```

The bootstrap 95% confidence interval of \[21.71,23.36\] is nearly identical to the t test 95% confidence interval of \[21.73,23.34\]. This indicates that for the mean, the bootstrap provides a very reliable approximation of the traditional t-distribution methods.

------------------------------------------------------------------------

**(e) Based on this data set, provide an estimate** $\hat{\mu}_{med}$**for the median value of medv in the population.**

```{r}
med_hat <- median(Boston$medv)
med_hat
```

The estimate for the population median of medv is the sample mean:

$\hat{\mu}_{med}≈21.2$ .

------------------------------------------------------------------------

**(f) We now would like to estimate the standard error of** $\hat{\mu}_{med}$**. Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your findings**.

```{r}
boot_median <- function(data, index){
  median(data[index])
}
boot_median_results <- boot(Boston$medv,
                            boot_median,
                            R=1000)
sd(boot_median_results$t)
```

I got a standard error of 0.38. The median is actually *slightly more stable*(lower SE) than the mean for this specific dataset.

------------------------------------------------------------------------

**(g) Based on this data set, provide an estimate for the tenth per- centile of medv in Boston census tracts. Call this quantity ˆ µ0.1. (You can use the quantile() function.)**

```{r}
percentile_10_hat <- quantile(Boston$medv, 0.1)
percentile_10_hat
```

The tenth percentile is approx 12.75.

------------------------------------------------------------------------

**(h) Use the bootstrap to estimate the standard error of ˆ µ0.1. Comment on your findings.**

```{r}
boot.fn_10 <- function(data, index) {
  return(quantile(data[index], 0.1))
}

set.seed(1)
boot_10 <- boot(Boston$medv, boot.fn_10, R = 1000)
boot_10
```

The bootstrap estimate of the standard error of the 10th percentile is approx. 0.4768. This is larger than the standard error of the mean and median, indicating that extreme quantiles tend to have greater variability.

------------------------------------------------------------------------

## 3-Least Squares is MLE

part1! Equivalence of Maximum Likelihood and Least Squares

**Show that in the case of linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and** $C_p$ **and AIC are equivalent.**

*Consider linear regression model:*

$Y_i = X_i\beta + \varepsilon_i$ *where errors are indep. and* $\varepsilon_i \sim N(0,\sigma^2)$.

then

$Y_i \sim N(X_i\beta,\sigma^2)$.

The likelihood function is

$f(Y_i | x_i, \beta, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(Y_i - x_i^T\beta)^2}{2\sigma^2} \right)$ . Since the observations are indepedent, the likelihood $L(\beta, \sigma^2)$ is the product of the individual densities:

$$L(\beta, \sigma^2) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(Y_i - x_i^T\beta)^2}{2\sigma^2} \right)$$

Taking the natural log. to get the log-likelihood:

$$\ell(\beta, \sigma^2) = \sum_{i=1}^{n} \left[ \log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) - \frac{(Y_i - x_i^T\beta)^2}{2\sigma^2} \right]$$

$$
\ell(\beta, \sigma^2) = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (Y_i - x_i^T\beta)^2
$$

To find the MLE estimate of beta hat, we maximize $\ell(\beta, \sigma^2)$ with respect to beta,

$$
\text{argmax}_{\beta} \ell(\beta, \sigma^2) = \text{argmin}_{\beta} \sum_{i=1}^{n} (Y_i - x_i^T\beta)^2
$$

sooo this summation is the residual sum of the squares (RSS). Since the $\beta$ that maximizes the likelihood is the same $\beta$ that minimizes RSS, MLE and Least Squares are equiv. under gaussian assumption!!!!!!!!

------------------------------------------------------------------------

part 2! equivalence of $C_p$ and AIC

For a linear model with *d* predictors, C_p and AIC defined as:

$C_p = \frac{1}{n}(RSS + 2d\hat{\sigma}^2)$

$AIC = -2\ell(\hat{\beta}, \hat{\sigma}^2) + 2d$

Ok, substituting maximum log-likelihood into the AIC formula for a gaussian model:

$AIC = \frac{RSS}{\hat{\sigma}^2} + n\log(2\pi\hat{\sigma}^2) + 2d$

If we treat estimate of error variance $\hat{\sigma}^2$ as a constant from full model, we can see that:

$AIC \cdot \hat{\sigma}^2 \approx RSS + 2d\hat{\sigma}^2 + \text{constant}$

we divide by n and ...

$\frac{AIC \cdot \hat{\sigma}^2}{n} \approx \frac{1}{n}(RSS + 2d\hat{\sigma}^2) = C_p$

Since $AIC$ and $C_p$ are linear transformations of each other, they will select the same "best" model. (AKA equiv criteria for model selection!!).

------------------------------------------------------------------------

## 4-6.6.1 ISL Exercise

**We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain p+ 1 models, containing 0,1,2,...,p predictors. Explain your answers:**

**(a) Which of the three models with k predictors has the smallest training RSS?**

Best Subset Selection, It evaluates every possible combination of *k* predictors out of the total *p* available and forward and backward stepwise selection are both only exploring nesting subsets so they may miss the global minimum for training RSS. So best subset selection will always have smallest straining RSS.

------------------------------------------------------------------------

**(b) Which of the three models with k predictors has the smallest test RSS?**

It depends. Training and test RSS are not the same so the test error depends on variance, bias, data, etc. We would need the specific dataset.

------------------------------------------------------------------------

**(c) True or False:**

**i. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by forward stepwise selection.**

True. Forward stepwise selection adds one predictor at a time, so the k-variable model is always a subset of the (k+1)-variable model.

**ii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k+ 1)-variable model identified by backward stepwise selection.**

True. Backward stepwise selection removes one predictor at a time, so the k-variable model is always a subset of the (k+1)-variable model.

**iii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k+ 1)-variable model identified by forward stepwise selection.**

False. Forward and backward stepwise selection may choose different predictors.

**iv. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by backward stepwise selection.**

False. Forward and backward stepwise selection may select different predictors.

**v. The predictors in the k-variable model identified by best subset are a subset of the predictors in the (k+ 1)-variable model identified by best subset selection.**

False. The best subset model with k predictors is not necessarily a subset of the best subset model with k+1 predictors.

------------------------------------------------------------------------

## 5-6.6.3 ISL Exercise

**Suppose we estimate the regression coeﬃcients in a linear regression model by minimizing**

![](images/clipboard-3764750715.png){width="278"}

**for a particular value of s. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer.**

(a) As we increase s from 0, training RSS will steadily decrease. When you allow a bigger s , the model has more freedom to fit the training data. So the training error can only go down or stay the same.

(b) test RSS will D=decrease initially, and then eventually start increasing in a U shape. Initially at s=0, the model is underfit (high bias). Increasing $s$ allows the model to capture the **actual** relationship in the data, reducing test error. EVENTUALLY, once s becomes too big, the model starts to overfit the noise (high variance).

(c) variance will steadily increase because more flexibility means the model reacts more to the training data = makes predictions **more sensitive**, so variance goes up!

(d) squared bias will steadily decrease because When s is small, the model is too simple = high bias. As s grows, the model can match the true pattern better = bias goes down.

(e) irreducible error is constant because it's is the noise inherent in the system that no model, no matter how perfect.

------------------------------------------------------------------------

## 6-6.6.4 ISL Exercise


![](images/clipboard-4042134813.png){width="353"}

(a) training RSS will steadily increase. as $\lambda$ grows, coefficients are shrunk towards 0 which makes the model fit the training data worse.

(b) test RSS will decrease initially, and then eventually start increasing in a U shape. A small $\lambda$ means overfitting and high test error, a medium one is best fit and low test error and a large $\lambda$ is underfitting and a high test error.

(c) variance will steadily decrease because bigger $\lambda$ shrinks coefficients and stabilizes model so predictions are less sens. to trainign data = variance goes down.

(d) squared bias will steadily increase because more shrinkage pulls coefficients away from true values = increases bias!

(e) irreducible error is constant because it's is the noise inherent in the system that no model, no matter how perfect.

------------------------------------------------------------------------

## 7-6.6.5 ISL Exercise

**It is well-known that ridge regression tends to give similar coeﬃcient values to correlated variables, whereas the lasso may give quite different coeﬃcient values to correlated variables. We will now explore this property in a very simple setting. Suppose that n = 2, p = 2, x11 = x12, x21 = x22. Furthermore, suppose that y1 +y2 = 0 and x11 +x21 = 0 and x12 +x22 = 0, so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero: β0 = 0.**

**(a) Write out the ridge regression optimization problem in this setting.**

$\text{Minimize } \sum_{i=1}^{2} \left( y_i - \beta_1 x_{i1} - \beta_2 x_{i2} \right)^2 + \lambda (\beta_1^2 + \beta_2^2)$

Expanding sum given values gives us...

$(y_1 - \beta_1 x_{11} - \beta_2 x_{11})^2 + (y_2 - \beta_1 x_{21} - \beta_2 x_{21})^2 + \lambda (\beta_1^2 + \beta_2^2)$.

------------------------------------------------------------------------

**(b) Argue that** $\hat{\beta}_1 = \hat{\beta}_2$ **in Ridge.**

The predictors are identical so swapping $\hat{\beta}_1$ and $\hat{\beta}_2$ does not change the fit. Ridge penalizes their squared sum so for a fixed sum where the predictors = c, the smallest value of their squares occurs when they are identical.

------------------------------------------------------------------------

**(c) Write out the lasso optimization problem in this setting.**

Lasso minimizes:

$\sum_{i=1}^{2} \left( y_i - \beta_1 x_{i1} - \beta_2 x_{i2} \right)^2 + \lambda (|\beta_1| + |\beta_2|)$

so using the fact that the predictors are identical:
$(y_1 - (\beta_1 + \beta_2) x_{11})^2 + (y_2 - (\beta_1 + \beta_2) x_{21})^2 + \lambda (|\beta_1| + |\beta_2|)$

------------------------------------------------------------------------

**(d) Argue that in this setting, the lasso coeﬃcients β1 and β2 are not unique—in other words, there are many possible solutions to the optimization problem in (c). Describe these solutions.**

The model depends only on $β1​+β2$ so many coefficient pairs can produce the same predictions. ​(like if they were both c/2). They just need to have the same penalty but multiple solutions **do exist**.

------------------------------------------------------------------------

## 8-6.6.11 ISL Exercise

```{r}
#loading data
library(ISLR2)
data(Boston)

set.seed(1)
```

We predict:

```{r}
y = Boston$crim
X = Boston[, -1]
```

Ok our train/test split:

```{r}
train = sample(1:nrow(Boston), nrow(Boston)/2)

y.train = Boston$crim[train]
y.test = Boston$crim[-train]

X.train = model.matrix(crim~., Boston)[train,]
X.test = model.matrix(crim~., Boston)[-train,]
```

Least Squares:

```{r}
lm.fit = lm(crim~., data=Boston, subset=train)

pred.lm = predict(lm.fit, Boston[-train,])

rmse.lm = sqrt(mean((pred.lm - y.test)^2))

rmse.lm
```

Ridge:

```{r}
library(glmnet)

ridge.fit = cv.glmnet(X.train, y.train, alpha=0)

best.lambda.ridge = ridge.fit$lambda.min

ridge.pred = predict(ridge.fit,
                     s=best.lambda.ridge,
                     newx=X.test)

rmse.ridge = sqrt(mean((ridge.pred - y.test)^2))

rmse.ridge
```

Lasso:

```{r}
lasso.fit = cv.glmnet(X.train, y.train, alpha=1)

best.lambda.lasso = lasso.fit$lambda.min

lasso.pred = predict(lasso.fit,
                     s=best.lambda.lasso,
                     newx=X.test)

rmse.lasso = sqrt(mean((lasso.pred - y.test)^2))

rmse.lasso
```

CV RMSE (use cross validation errors):

```{r}
cv.lm = sqrt(mean(residuals(lm.fit)^2))

cv.ridge = sqrt(min(ridge.fit$cvm))

cv.lasso = sqrt(min(lasso.fit$cvm))
```

FINAL TABLE FORMAT!

```{r}
data.frame(
Method=c("LS","Ridge","Lasso"),
CV_RMSE=c(cv.lm,cv.ridge,cv.lasso),
Test_RMSE=c(rmse.lm,rmse.ridge,rmse.lasso)
)
```

------------------------------------------------------------------------

\(b\) Ridge is the best model because it had the lowest test RMSE (6.338) and a lower test RMSE indicates better predictions. Lasso performs slightly worse and the least squares performs the worst so ride is the preferred model for predicting crime rate in the Boston data.

\(c\) No, the chosen model does not necessarily involve all features. Ridge regression shrinks coefficients towards zero, which reduces overfitting even when all predictors included. Lasso, for example performs variable selection so you can check which were excluded.

------------------------------------------------------------------------

## 
9- Bonus Question

The least squares error estimate $\hat{\beta}$ is chosen specifically to minimize training error:

$\hat{\beta} = \text{arg min}_{\beta} R_{\text{train}}(\beta)$

so.... $\hat{\beta}$ is optimized for training data. but the test data is new data so its not optimized for it.

1\) Training error minimized

By def, $Rtrain​(β^​)≤Rtrain​(β)$ for any new β. so, $\hat{\beta}$ gives the smallest possible training error.

2\) Test error uses NEWWWW data

Test error is:

$R_{\text{test}}(\beta)=\frac{1}{M}\sum_{i=1}^{M}\left(\tilde{y}_i - \beta^T \tilde{x}_i\right)^2$

and these observations were not used to fit the model so beta hat does not minimize this quantity.

—The test data are drawn from the same population as the training data but are independent of the training set used to calculate $\hat{\beta}$. Mathematically, the expected test error is equivalent to the expected training error evaluated at a fixed, true population parameter $\beta^*$

$E[R_{\text{test}}(\hat{\beta})] = E[R_{\text{train}}(\beta^*)]$

Since $\hat{\beta}$ is chosen specifically to minimize R_train, it gets both the signal and noise unique to the training sample. Because $\hat{\beta}$ is the minimizer of the training error but not the test error, the following holds:

$E[R_{\text{train}}(\hat{\beta})] < E[R_{\text{train}}(\beta^*)] = E[R_{\text{test}}(\hat{\beta})]$
